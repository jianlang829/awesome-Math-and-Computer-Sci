### 特征值与特征向量的核心

- **定义**：对方阵 \( A \)，若有非零向量 \( v \) 和标量 \( \lambda \) 使  
  \[ A v = \lambda v \]  
  则 \( \lambda \) 为特征值，\( v \) 为对应特征向量。
- **本质**：揭示线性变换的“主轴”与缩放因子。

---

### 在计算机中的典型用途

1. **PCA 降维**（压缩、去噪）
2. **PageRank**（网页排序）
3. **图聚类**（Spectral Clustering）
4. **稳定性分析**（振动模态、CFL 条件）

---

### 极简可跑示例：PCA 降维（Python，≤10 行）

```python
import numpy as np, matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 生成 2D 椭圆数据
X = np.dot(np.random.randn(100,2), [[3,1],[1,2]])

# PCA 降为一维
pca = PCA(n_components=1)
X1 = pca.fit_transform(X)

# 还原观察
X_rec = pca.inverse_transform(X1)
plt.scatter(X[:,0], X[:,1], alpha=.5, label='origin')
plt.scatter(X_rec[:,0], X_rec[:,1], alpha=.5, label='1D PCA')
plt.legend(); plt.show()
```

**验证**：  
打印 `pca.components_` 即主方向（特征向量），`pca.explained_variance_` 为对应特征值；降维后数据几乎落在一条直线上，完成压缩。
